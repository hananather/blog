{"title":"Introduction to LLMs","markdown":{"yaml":{"title":"Introduction to LLMs","date":"2025-05-29","author":"Hanan Ather","description":"An introduction to Large Language Models and their applications","categories":["AI","Machine Learning","LLMs"],"format":{"html":{"toc":true,"toc-float":{"collapsed":false,"smooth-scroll":true},"toc-title":"Table of Contents","code-tools":true,"code-fold":true,"code-summary":"Show code","code-line-numbers":true,"highlight-style":"github"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nHow many of you have had a conversation like this with ChatGPT recently?\n```\n\nUser: \"Analyze the trends in our Q3 survey data. We had 15,000 responses \nabout housing affordability across different provinces.\"\n\nChatGPT: \"Based on your description, here are some key trends I'd expect \nto see in Q3 housing affordability data:\n\n1. Regional variations likely show BC and Ontario with highest concerns\n2. Age demographics probably reveal millennials most affected\n3. Income brackets under $75K likely report significant challenges\n4. Urban vs rural differences may show urban areas more impacted\n\nTo get specific insights, I'd recommend looking at cross-tabulations \nbetween income, location, and reported affordability stress levels...\"\n\n```\n\n**This highlights a fundamental challenge we face in 2025: LLMs are incredibly smart, but they're designed for human conversation, not system integration**\n\nIn our daily work at StatCan, we don't just need insights we need those insights to flow into our data pipelines, our reporting systems, our analysis workflows. We need to bridge this gap between conversational AI and operational systems.\n\nAPI versions of these same LLMs can eliminate this translation step entirely. Instead of getting human-readable text that you then have to interpret and act upon, you can get:\n- Structured data that flows directly into your analysis tools\n- Function calls that automatically trigger the right actions in your systems\n- Formatted outputs that match your reporting requirements exactly\n\nBefore:\n![image.png](attachment:ab846644-8598-42be-86dc-06e9ac6ced93.png)\n\nAsk ChatGPT question → 2. Read response → 3. Interpret manually → 4. Write code/queries → 5. Execute actions → 6. Format results\n\n![image.png](attachment:c6ea8749-916d-465e-8762-7fbbd49f952a.png)\n\nAfter:\nSend API request → 2. LLM returns structured data + triggers actions → 3. Results automatically formatted and delivered\n\n**The key insight is this: The same AI capabilities you use in chat can be repurposed as intelligent middleware in your data systems.**\nLet me show you exactly what this looks like in practice, starting with a simple example that will make the concept crystal clear.\n\n# Text and prompting\nThe first concept we will go over is learning how to prompt a model to generate text (prompt = instruction). \nWe can use a large langugage model to generate text from a prompt, the same way as we you might use ChatGPT. Models can generate almost any kind of text response:\n1. code\n2. mathematical equations\n3. structured JSON data\n4. Human-like prose\n\nAn array of content generated by the model is in the `choices` property of the response. In this example, we have just one output which looks like this:\n``` json\n[\n    {\n        \"index\": 0,\n        \"message\": {\n            \"role\": \"assistant\",\n            \"content\": \"Statistics Canada is the national statistical agency of Canada, responsible for producing and disseminating official statistics about the country’s population, economy, resources, and society. Established in 1971, it operates under the *Statistics Act* to provide reliable and impartial data to support decision-making by governments, businesses, and the public. Its key activities include conducting nationwide surveys and censuses, such as the Census of Population and Census of Agriculture. Statistics Canada emphasizes confidentiality and data quality, playing a crucial role in informing policy, research, and public understanding of Canada’s trends and developments\".\n            \"refusal\": null\n        },\n        \"logprobs\": null,\n        \"finish_reason\": \"stop\"\n    }\n]\n```\n\nIn addition to plain text, you can also return structured data in JSON format - this feature is called Structured Outputs.\n\n# Model Optimization\nLLMs output is non-deterministic and model behaviour changes between snaphots and families. So as developers of AI applications we must constantly measure and tune the performance of an LLM applications to ensure they are getting the best results.\n\nOptimizing model ouputs require a combination of **eval, prompt engineering, and fine-tuning**, creating a flywheel of feedback that leads to better prompts and better training data for fine-tuning. The optimization process usually goes something like this\n\n1. Write evals that measure model output, establishing a baseline for performance\n2. Prompt the model, providng relevant context data and instrutions.\n3. For some usecases, it may be desirable to fine-tune a model for specific task\n4. Run evals using test data that is representive of real world inputs. Measure the performance of your prompt and fine-tune model.\n5. Tweak your prompt or fine-tuning dataset based on eval feedback\n6. Repeat the loop continiously to improve your model results\n\n\n\n![image.png](attachment:d22b0c14-5344-49f3-ad05-dd90246237ba.png)\n\n### Choosing a model\nA key choice to make when generating content through the API is which model you want to use. The `model` parameter of the code sample above. Here are a few factors to consider when choosing a model for text generation. \n- **Reasoning model** generate an internal chain of thought to analyze the input prompt, and excel at understanding complex task and multi-step planning. They are also generally  more slower and more expensive to use\n- **Large and small (mini or nano) models** offer trade-offs for speed, cost and intelligence. Large models are more effective at understanding prompts and solving problems across domains while small models are generally faster and cheaper to use\n\n### Prompt Engineering\nPrompt engineering is the process of writing effective instructions for a model, such that it consistently generates content that meets your requirements.\n\nBecause the content generated from a model is non-deterministic, its a combination of art and science to build a prompt that will generate content in the format you want. However, there are a number of techniques and best practices we can apply to get good results from a model.\nSome prompt engineering techniques will work with every model, but different model types (like reasoning versus GPT models) might need to be prompted differnlty to produce the best results. Even different snapshots of models with the same family could produce differnt results. \n\nSo as you are building more complex AI applicaitons, its strongly recomended to:\n- pin production applications to specific model snapshots to ensure consistent behaviour\n- building evals that will measure the behaviour of your prompts, so that we can monitor the performance of prompts as you iterate on them, or when you change and update model version.\n\nNow lets examine some tools and techniques available to me to construct prompts. \nWe can provide instructions (prompts) to the model with differing levels of authority using `message roles`.\n\n- `DEVELOPER` messages are instructions provided by the application developer, prioritized ahead of `user` messsages\n- `USER` messages are instructions provided by an end user, prioritized behind developer message\n- `ASSISTANT` messages are generated by the model\n\n\n![image.png](attachment:69b89a94-71f7-4635-a861-bec3c56ef454.png)\n\nA mult-turn conversations may consist of several messages of these types, along with other content types provided by both the user and the model.\n\nWe can think about the `developer` and `user` messages like a function and its arguments in a programming langauge.\n- `developer` messages provide the system's rules and business logic, like a function definition\n- `user` messages provide inputs and configurations to which `developer` message instructions are applied, like arguments to a function.\n\nWhen writing `developer` and `user` messages, you can help the model understand logical boundaries of your prompt and the context data.\n\n## Conversation state\nThis naturally leads us to the idea of how to manage conversation state during a model interation. There are a few ways to manage conversation state, which is important for preserving information across multiple messages or turns in a conversation. \n\n### Manually manage the conversation state\n\nGenerally for most APIs each text generation request is indepent and stateless (unless we are specific APIs like the Assistants API), but we can still implement multi-turn conversations by providing additional messages as parameters to our text generation request.\n\nBy using alternating `user` and `assistant` messages, we can capture the previous state of conversation in one request to the model. \nTo manually share context across generated responses, you include the model's previous response output as input and append that input to your next request. \n\nIn the following example, we ask the model about Canadian demographics, followed by a request for provincial data. Appending previous responses to new requests in this way helps ensure conversations feel natural and retain the context of previous interactions.\n\n### Including relevant context information\nIts often useful to include additional context information the model can use to generate a response within the prompt you give to the model. There are few common resons to do this:\n- to give the model access to any data data outside the data set the model was trained on.\n- to constrain the model's response to a specific set of resources that you have determined will be most beneficial\n\nThis technique of adding additional releveant context to the model generation request is sometimes called **retrieval augmented generation (RAG)**. You can add additional context to the prompt in many different ways, from querying from a vector database and including the text back into the prompt, or by APIs provide a bult-in file search tool to generate content based on uploaded documents.\n\n### Managing the context window\nModels can only handle so much data within the context they consider during a generation request. (Mentioned needle in haystack metric). This memory limit is called a context window, which is defined in terms of tokens (chunks of data you pass in, from text, images or audio)\n\nUnderstanding the context window allows us to successfully create threaded conversations and manage state across model interactions.\n\nThe **context window** is the maximum number of tokens that be used in a single request. The max tokens include input, output and reasoning token.\n\nAs the inputs become more complex or we include more turns in a conversation, we will need to consider both **output token** and **context window** limits. Model input and outputs are metered in tokens, which at high level prased from inputs to analyze their content and intent and assembled to render logical outputs. \n\n- **Output tokens** are tokens generated by a model in response to a prompt. Each model has different limits for output tokens.\n- A **context window** describes the total tokens that can be used for both input and output tokens (and for some models reasoning tokens)\n\nIf we create a very large prompt, often by including extra context, data, or examples of the model, we can run the risk of exceeding the allocated context window for a model, and this can result in truncated outputs. \n![image.png](attachment:96349366-e8bd-41a3-a92c-a55483f1c73c.png)\n\n# Structured Outputs\n\nOften in production, we need models to generate outputs following certain formats. Structured outputs are curcial. They are crucial for the following two scenarios: \n\n1. **Tasks requiring structued outputs.** The most common category of tasks in this scenario is semantic parsing. Semantic parisng involves converting natural language into structured, machine readble lanauge.\n    - Text-to-SQL is an example of semantic parsing, where the outputs must be valid SQL queries.\n2. **Tasks whose outputs are used by downstream applications**. In this scenario the outputs are used by other applications. For example, if you use an AI model to write an email, the email itself doesn't have ot be structued. However, a downstream application using this email might need to be in a specific format. For example: a JSON documet with specifc keys such as `{\"title\": [TITLE], \"body\": [EMAIL BODY]}`.\n  **This is especially important for agentic workflows where a model's outputs are often passed as inputs into tools that the model can use.**\n\n#### Constrained sampling\nConstrained sampling is a technique for guiding the generation of text toward certain constraints. It is typically followed by structured output tools.\n\nAt a high level, to generate a token, the model samples among values that meet the constraints. Recall that,  to generate a token, our model first outputs a logit vector, each logit corresponds to one possible token. Constrained sampling filters this logit vector to keep only the tokens that meet the constraints. It then samples form these valid tokens. \n\n![image.png](attachment:b92ea2ad-2a1f-4cf9-a723-7795e2969cb6.png)\n\nIn this example the constraint is straightforward to filter for. However, most caases aren't that straightforward. You need to have a grammar that specifies what is and isn't allowed. For example, JSON grammar dictates that after `{`, you can't have another `{` unless its part of a string, as in `{\"key\": \"{{string}}\"}`. \n\nBuilding out that grammar and incorporating it into the sampling process is nontrivial. Because each output format, JSON, YAML, regex,CSV, and so on, needs its own grammar. So constrained sampling is limited to formats whose grammars are supported by external tools. **Grammar verification can also increase generation latency (Brandon T. Willard, 2024).** Some are against contrained sampling becasue they believe that the resources needed for constrained sampling are better invested in training models to follow instructions via prompt engineering or fine-tuning. \n\n\n\n### Finetuning (feature-based transfer)\nFor certian task, we can also guarantee the models output format by modifying the model's architecture before finetuning. For example, for classification, you can append a classifier head to the foundation model's architecture to make sure the model outputs only one of the pre-specified classes. This technique is also called **feature-based transfer** and is just one approach among other transfer learning techniques. \n\nDuring finetuning, we can retrain the whole model-end-to-end or part of the model, such as the classifer head. End-to-end training requires more resources, but promises better performance. \n\n![image.png](attachment:d3dfe8c3-a372-4b07-b7d7-cb70cadc4f2b.png)\n\n\n---\n\n## Core Idea\nThe core idea is that we can configure our LLM API for structured output instead of unstructured text. Period. This allows precise extraction and standardization of information for further processing. Period. For example, we can use the structured output to extract information from our resumes or standardize them to build a structured database. Let's go over a couple of examples.\n\nNow, there are a couple of ways to get structured outputs out of a model. The first one that was introduced when structured outputs became a thing back 2023 was when I first discovered them. Structured outputs were a feature that ensured that a model would always generate responses that adhere to a JSON schema. So at that time, we had to give a JSON schema if you wanted to get structured outputs. However now, and more recently, APIs like for example, the OpenAI API just has structured outputs as a built-in feature. And in addition to supporting just JSON schema in the REST API, the OpenAI SDKs for Python and JavaScript also make it easy to define object schema using Pydantic and Zod if you're using TypeScript respectively. So let me give an example of how you can extract information from unstructured output that conforms to a schema defined in code.\n\n\n\n\n---\n\n\n### Generating `enum` values\n\nBefore we explore how enums work with structured outputs, let's clarify what enums are in programming languages, particularly Python.\n\n\nAn enum (short for enumeration) is a way to create a set of named constants that represent a fixed collection of related values. Think of it as defining a specific list of allowed options that a variable can take.\n\n```\nfrom enum import Enum\n\nclass Status(Enum):\n    PENDING = \"pending\"\n    APPROVED = \"approved\" \n    REJECTED = \"rejected\"\n    IN_REVIEW = \"in_review\"\n\n```\nIn this example, `Status` can only be one of these four predefined values. You can't accidentally set it to \"aproved\" (misspelled) or \"maybe\" – it must be one of the defined options.\n\nEnums become incredibly valuable when working with LLMs in real-world applications because they solve fundamental problems with unconstrained text generation.\n\n\nWhen LLMs generate free-form text responses, they operate in an infinite possibility space. Even for simple categorical decisions, models might express the same concept through countless linguistic variations, synonyms, or formatting differences. This variability creates a mismatch between the deterministic requirements of software systems and the inherently creative nature of language models.\n\n\n\nEnums transform open-ended generation into constrained selection. Instead of choosing from infinite possibilities, the model must map its understanding to one of your predefined categories. \n\nBy forcing the model to use your exact terminology, enums ensure that conceptually identical responses are represented identically in your system. The model's internal understanding gets translated into your application's vocabulary.\n\nDeterministic Interfaces: Enums create predictable contracts between your LLM and downstream systems. Rather than parsing and interpreting varied text outputs, your application receives known constants that can be handled with simple conditional logic.\n\nData Integrity at Scale: In production environments, consistency compounds over time. Enums prevent the gradual degradation of data quality that occurs when small variations accumulate across thousands or millions of LLM interactions.\n\n\n\n# Function Calling\n\nFunction calling lets us connect our models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. \n\nBasically the idea is that we can give the model access to our own custom code through a function calling. And based on the context that the model has been provided via prompt and messages, the model may decide to call these functions instead of generating the output text.\n\nThen the idea is that we'll execute the function code, send back the results and the model will incorporate them to its final response.\n\n\n## How Function Calling Works\n\nSo, as we will see, function calling just simply builds on this idea of structured outputs, where function calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the steps:\n\n\n**Step 1: Define Function Declaration**\n\nWe define a function and its declaration within our application code that allows the user to set light And make API requests So this function could hypothetically call external services or APIs for instance\n\n\n\n\n\n1. Define function declarations. So we define the function declaration in our application code. Function declarations describe the function's name, parameters, and purpose to the model.\n2. Call the LLM with function declarations. Send user prompt along with the function declarations to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n3. Execute function code, bracket R responsibility, bracket close. The model does not execute the function itself. Is your application's responsibility to process the response back and check for function call?\n\n    - If yes, then extract the name and the arguments of the function and execute the corresponding function in your application.\n    - if No, the model has provided a direct text response to the prompt.\n4. If a function was executed, we capture the result and send it back to the model in a subsequent turn of conversation. And it will use the result to generate a response that incorporates the information from the function call.\n\nNow note that this process can be repeated over multiple turns. So this can allow for complex interactions and workflows. It's possible that even after providing the function call output, the model decides that it needs to use another function. And then it continues on using the calling function until it's satisfied. And the model can also support calling multiple functions in a single turn, which is called parallel function calling. And in sequence, which is called compositional function calling.\n\n# Example \nLet’s walk through a simple—but very representative—example of how function calling works.\nImagine you’re building a AI system that, for this really simple example lets that it  helps users ask questions about population estimates.\n\n\nBehind the scenes, we already have systems that store this kind of data, Business Registar, Building Registar, any sort of database, or it can be an external API.\n\nFor now, let’s pretend we have a Python function that just returns population counts based on a lookup. It’s stand in for our real data source.\n\nIn a real deployment, get_population_estimate could easily wrap:\n- A call to the StatsCan API,\n- A SQL query on a secure census warehouse,\n- Or a simple pandas.read_csv() from a shared directory.\n\nWhat matters is that we’ve exposed the function and described its signature in a way that the model understands—now, the model can decide when to call it and how to call it, but it won’t hallucinate the output.\n\n\n**Step 1 – Implement and document the function**\n\n**Step 2 – Publish the function schema to the model**\n\nWhen you create a chat completion you supply a tools list. Each entry is a JSON description of a function:\n\n**Step 3 – Start the conversation**:\nKick off a chat session with the tools attached.\n\n\n**Step 4 – Model emits a function call**\nIf LLM decides the function is relevant, the first response in choices[0].message will include a tool_calls array:\n\n\n``` python\n[\n    ChatCompletionMessageToolCall(id='call_aFCdjVWwL4K9rN0bZzFfgTpx',\n    function=Function(arguments='{\"geography\":\"Ontario\",\"year\":2021}',\n                      name='get_population_estimate'), type='function')]\n```\n\n**Step 5 – Execute the function**:\n\nYour code parses the arguments, runs the function, and obtains a result:\n\n**Step 6 – Feed the result back to the model**\n\nAppend two new messages to the running list:\n\n1. The original function-call message returned by the model (so GPT keeps its own conversational state).\n2. A tool-role message whose content is the JSON result and whose tool_call_id matches the earlier id.\n\n\nStep 7 – Ask the model to finish the answer\n","srcMarkdownNoYaml":"\n\n# Introduction\n\nHow many of you have had a conversation like this with ChatGPT recently?\n```\n\nUser: \"Analyze the trends in our Q3 survey data. We had 15,000 responses \nabout housing affordability across different provinces.\"\n\nChatGPT: \"Based on your description, here are some key trends I'd expect \nto see in Q3 housing affordability data:\n\n1. Regional variations likely show BC and Ontario with highest concerns\n2. Age demographics probably reveal millennials most affected\n3. Income brackets under $75K likely report significant challenges\n4. Urban vs rural differences may show urban areas more impacted\n\nTo get specific insights, I'd recommend looking at cross-tabulations \nbetween income, location, and reported affordability stress levels...\"\n\n```\n\n**This highlights a fundamental challenge we face in 2025: LLMs are incredibly smart, but they're designed for human conversation, not system integration**\n\nIn our daily work at StatCan, we don't just need insights we need those insights to flow into our data pipelines, our reporting systems, our analysis workflows. We need to bridge this gap between conversational AI and operational systems.\n\nAPI versions of these same LLMs can eliminate this translation step entirely. Instead of getting human-readable text that you then have to interpret and act upon, you can get:\n- Structured data that flows directly into your analysis tools\n- Function calls that automatically trigger the right actions in your systems\n- Formatted outputs that match your reporting requirements exactly\n\nBefore:\n![image.png](attachment:ab846644-8598-42be-86dc-06e9ac6ced93.png)\n\nAsk ChatGPT question → 2. Read response → 3. Interpret manually → 4. Write code/queries → 5. Execute actions → 6. Format results\n\n![image.png](attachment:c6ea8749-916d-465e-8762-7fbbd49f952a.png)\n\nAfter:\nSend API request → 2. LLM returns structured data + triggers actions → 3. Results automatically formatted and delivered\n\n**The key insight is this: The same AI capabilities you use in chat can be repurposed as intelligent middleware in your data systems.**\nLet me show you exactly what this looks like in practice, starting with a simple example that will make the concept crystal clear.\n\n# Text and prompting\nThe first concept we will go over is learning how to prompt a model to generate text (prompt = instruction). \nWe can use a large langugage model to generate text from a prompt, the same way as we you might use ChatGPT. Models can generate almost any kind of text response:\n1. code\n2. mathematical equations\n3. structured JSON data\n4. Human-like prose\n\nAn array of content generated by the model is in the `choices` property of the response. In this example, we have just one output which looks like this:\n``` json\n[\n    {\n        \"index\": 0,\n        \"message\": {\n            \"role\": \"assistant\",\n            \"content\": \"Statistics Canada is the national statistical agency of Canada, responsible for producing and disseminating official statistics about the country’s population, economy, resources, and society. Established in 1971, it operates under the *Statistics Act* to provide reliable and impartial data to support decision-making by governments, businesses, and the public. Its key activities include conducting nationwide surveys and censuses, such as the Census of Population and Census of Agriculture. Statistics Canada emphasizes confidentiality and data quality, playing a crucial role in informing policy, research, and public understanding of Canada’s trends and developments\".\n            \"refusal\": null\n        },\n        \"logprobs\": null,\n        \"finish_reason\": \"stop\"\n    }\n]\n```\n\nIn addition to plain text, you can also return structured data in JSON format - this feature is called Structured Outputs.\n\n# Model Optimization\nLLMs output is non-deterministic and model behaviour changes between snaphots and families. So as developers of AI applications we must constantly measure and tune the performance of an LLM applications to ensure they are getting the best results.\n\nOptimizing model ouputs require a combination of **eval, prompt engineering, and fine-tuning**, creating a flywheel of feedback that leads to better prompts and better training data for fine-tuning. The optimization process usually goes something like this\n\n1. Write evals that measure model output, establishing a baseline for performance\n2. Prompt the model, providng relevant context data and instrutions.\n3. For some usecases, it may be desirable to fine-tune a model for specific task\n4. Run evals using test data that is representive of real world inputs. Measure the performance of your prompt and fine-tune model.\n5. Tweak your prompt or fine-tuning dataset based on eval feedback\n6. Repeat the loop continiously to improve your model results\n\n\n\n![image.png](attachment:d22b0c14-5344-49f3-ad05-dd90246237ba.png)\n\n### Choosing a model\nA key choice to make when generating content through the API is which model you want to use. The `model` parameter of the code sample above. Here are a few factors to consider when choosing a model for text generation. \n- **Reasoning model** generate an internal chain of thought to analyze the input prompt, and excel at understanding complex task and multi-step planning. They are also generally  more slower and more expensive to use\n- **Large and small (mini or nano) models** offer trade-offs for speed, cost and intelligence. Large models are more effective at understanding prompts and solving problems across domains while small models are generally faster and cheaper to use\n\n### Prompt Engineering\nPrompt engineering is the process of writing effective instructions for a model, such that it consistently generates content that meets your requirements.\n\nBecause the content generated from a model is non-deterministic, its a combination of art and science to build a prompt that will generate content in the format you want. However, there are a number of techniques and best practices we can apply to get good results from a model.\nSome prompt engineering techniques will work with every model, but different model types (like reasoning versus GPT models) might need to be prompted differnlty to produce the best results. Even different snapshots of models with the same family could produce differnt results. \n\nSo as you are building more complex AI applicaitons, its strongly recomended to:\n- pin production applications to specific model snapshots to ensure consistent behaviour\n- building evals that will measure the behaviour of your prompts, so that we can monitor the performance of prompts as you iterate on them, or when you change and update model version.\n\nNow lets examine some tools and techniques available to me to construct prompts. \nWe can provide instructions (prompts) to the model with differing levels of authority using `message roles`.\n\n- `DEVELOPER` messages are instructions provided by the application developer, prioritized ahead of `user` messsages\n- `USER` messages are instructions provided by an end user, prioritized behind developer message\n- `ASSISTANT` messages are generated by the model\n\n\n![image.png](attachment:69b89a94-71f7-4635-a861-bec3c56ef454.png)\n\nA mult-turn conversations may consist of several messages of these types, along with other content types provided by both the user and the model.\n\nWe can think about the `developer` and `user` messages like a function and its arguments in a programming langauge.\n- `developer` messages provide the system's rules and business logic, like a function definition\n- `user` messages provide inputs and configurations to which `developer` message instructions are applied, like arguments to a function.\n\nWhen writing `developer` and `user` messages, you can help the model understand logical boundaries of your prompt and the context data.\n\n## Conversation state\nThis naturally leads us to the idea of how to manage conversation state during a model interation. There are a few ways to manage conversation state, which is important for preserving information across multiple messages or turns in a conversation. \n\n### Manually manage the conversation state\n\nGenerally for most APIs each text generation request is indepent and stateless (unless we are specific APIs like the Assistants API), but we can still implement multi-turn conversations by providing additional messages as parameters to our text generation request.\n\nBy using alternating `user` and `assistant` messages, we can capture the previous state of conversation in one request to the model. \nTo manually share context across generated responses, you include the model's previous response output as input and append that input to your next request. \n\nIn the following example, we ask the model about Canadian demographics, followed by a request for provincial data. Appending previous responses to new requests in this way helps ensure conversations feel natural and retain the context of previous interactions.\n\n### Including relevant context information\nIts often useful to include additional context information the model can use to generate a response within the prompt you give to the model. There are few common resons to do this:\n- to give the model access to any data data outside the data set the model was trained on.\n- to constrain the model's response to a specific set of resources that you have determined will be most beneficial\n\nThis technique of adding additional releveant context to the model generation request is sometimes called **retrieval augmented generation (RAG)**. You can add additional context to the prompt in many different ways, from querying from a vector database and including the text back into the prompt, or by APIs provide a bult-in file search tool to generate content based on uploaded documents.\n\n### Managing the context window\nModels can only handle so much data within the context they consider during a generation request. (Mentioned needle in haystack metric). This memory limit is called a context window, which is defined in terms of tokens (chunks of data you pass in, from text, images or audio)\n\nUnderstanding the context window allows us to successfully create threaded conversations and manage state across model interactions.\n\nThe **context window** is the maximum number of tokens that be used in a single request. The max tokens include input, output and reasoning token.\n\nAs the inputs become more complex or we include more turns in a conversation, we will need to consider both **output token** and **context window** limits. Model input and outputs are metered in tokens, which at high level prased from inputs to analyze their content and intent and assembled to render logical outputs. \n\n- **Output tokens** are tokens generated by a model in response to a prompt. Each model has different limits for output tokens.\n- A **context window** describes the total tokens that can be used for both input and output tokens (and for some models reasoning tokens)\n\nIf we create a very large prompt, often by including extra context, data, or examples of the model, we can run the risk of exceeding the allocated context window for a model, and this can result in truncated outputs. \n![image.png](attachment:96349366-e8bd-41a3-a92c-a55483f1c73c.png)\n\n# Structured Outputs\n\nOften in production, we need models to generate outputs following certain formats. Structured outputs are curcial. They are crucial for the following two scenarios: \n\n1. **Tasks requiring structued outputs.** The most common category of tasks in this scenario is semantic parsing. Semantic parisng involves converting natural language into structured, machine readble lanauge.\n    - Text-to-SQL is an example of semantic parsing, where the outputs must be valid SQL queries.\n2. **Tasks whose outputs are used by downstream applications**. In this scenario the outputs are used by other applications. For example, if you use an AI model to write an email, the email itself doesn't have ot be structued. However, a downstream application using this email might need to be in a specific format. For example: a JSON documet with specifc keys such as `{\"title\": [TITLE], \"body\": [EMAIL BODY]}`.\n  **This is especially important for agentic workflows where a model's outputs are often passed as inputs into tools that the model can use.**\n\n#### Constrained sampling\nConstrained sampling is a technique for guiding the generation of text toward certain constraints. It is typically followed by structured output tools.\n\nAt a high level, to generate a token, the model samples among values that meet the constraints. Recall that,  to generate a token, our model first outputs a logit vector, each logit corresponds to one possible token. Constrained sampling filters this logit vector to keep only the tokens that meet the constraints. It then samples form these valid tokens. \n\n![image.png](attachment:b92ea2ad-2a1f-4cf9-a723-7795e2969cb6.png)\n\nIn this example the constraint is straightforward to filter for. However, most caases aren't that straightforward. You need to have a grammar that specifies what is and isn't allowed. For example, JSON grammar dictates that after `{`, you can't have another `{` unless its part of a string, as in `{\"key\": \"{{string}}\"}`. \n\nBuilding out that grammar and incorporating it into the sampling process is nontrivial. Because each output format, JSON, YAML, regex,CSV, and so on, needs its own grammar. So constrained sampling is limited to formats whose grammars are supported by external tools. **Grammar verification can also increase generation latency (Brandon T. Willard, 2024).** Some are against contrained sampling becasue they believe that the resources needed for constrained sampling are better invested in training models to follow instructions via prompt engineering or fine-tuning. \n\n\n\n### Finetuning (feature-based transfer)\nFor certian task, we can also guarantee the models output format by modifying the model's architecture before finetuning. For example, for classification, you can append a classifier head to the foundation model's architecture to make sure the model outputs only one of the pre-specified classes. This technique is also called **feature-based transfer** and is just one approach among other transfer learning techniques. \n\nDuring finetuning, we can retrain the whole model-end-to-end or part of the model, such as the classifer head. End-to-end training requires more resources, but promises better performance. \n\n![image.png](attachment:d3dfe8c3-a372-4b07-b7d7-cb70cadc4f2b.png)\n\n\n---\n\n## Core Idea\nThe core idea is that we can configure our LLM API for structured output instead of unstructured text. Period. This allows precise extraction and standardization of information for further processing. Period. For example, we can use the structured output to extract information from our resumes or standardize them to build a structured database. Let's go over a couple of examples.\n\nNow, there are a couple of ways to get structured outputs out of a model. The first one that was introduced when structured outputs became a thing back 2023 was when I first discovered them. Structured outputs were a feature that ensured that a model would always generate responses that adhere to a JSON schema. So at that time, we had to give a JSON schema if you wanted to get structured outputs. However now, and more recently, APIs like for example, the OpenAI API just has structured outputs as a built-in feature. And in addition to supporting just JSON schema in the REST API, the OpenAI SDKs for Python and JavaScript also make it easy to define object schema using Pydantic and Zod if you're using TypeScript respectively. So let me give an example of how you can extract information from unstructured output that conforms to a schema defined in code.\n\n\n\n\n---\n\n\n### Generating `enum` values\n\nBefore we explore how enums work with structured outputs, let's clarify what enums are in programming languages, particularly Python.\n\n\nAn enum (short for enumeration) is a way to create a set of named constants that represent a fixed collection of related values. Think of it as defining a specific list of allowed options that a variable can take.\n\n```\nfrom enum import Enum\n\nclass Status(Enum):\n    PENDING = \"pending\"\n    APPROVED = \"approved\" \n    REJECTED = \"rejected\"\n    IN_REVIEW = \"in_review\"\n\n```\nIn this example, `Status` can only be one of these four predefined values. You can't accidentally set it to \"aproved\" (misspelled) or \"maybe\" – it must be one of the defined options.\n\nEnums become incredibly valuable when working with LLMs in real-world applications because they solve fundamental problems with unconstrained text generation.\n\n\nWhen LLMs generate free-form text responses, they operate in an infinite possibility space. Even for simple categorical decisions, models might express the same concept through countless linguistic variations, synonyms, or formatting differences. This variability creates a mismatch between the deterministic requirements of software systems and the inherently creative nature of language models.\n\n\n\nEnums transform open-ended generation into constrained selection. Instead of choosing from infinite possibilities, the model must map its understanding to one of your predefined categories. \n\nBy forcing the model to use your exact terminology, enums ensure that conceptually identical responses are represented identically in your system. The model's internal understanding gets translated into your application's vocabulary.\n\nDeterministic Interfaces: Enums create predictable contracts between your LLM and downstream systems. Rather than parsing and interpreting varied text outputs, your application receives known constants that can be handled with simple conditional logic.\n\nData Integrity at Scale: In production environments, consistency compounds over time. Enums prevent the gradual degradation of data quality that occurs when small variations accumulate across thousands or millions of LLM interactions.\n\n\n\n# Function Calling\n\nFunction calling lets us connect our models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. \n\nBasically the idea is that we can give the model access to our own custom code through a function calling. And based on the context that the model has been provided via prompt and messages, the model may decide to call these functions instead of generating the output text.\n\nThen the idea is that we'll execute the function code, send back the results and the model will incorporate them to its final response.\n\n\n## How Function Calling Works\n\nSo, as we will see, function calling just simply builds on this idea of structured outputs, where function calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the steps:\n\n\n**Step 1: Define Function Declaration**\n\nWe define a function and its declaration within our application code that allows the user to set light And make API requests So this function could hypothetically call external services or APIs for instance\n\n\n\n\n\n1. Define function declarations. So we define the function declaration in our application code. Function declarations describe the function's name, parameters, and purpose to the model.\n2. Call the LLM with function declarations. Send user prompt along with the function declarations to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n3. Execute function code, bracket R responsibility, bracket close. The model does not execute the function itself. Is your application's responsibility to process the response back and check for function call?\n\n    - If yes, then extract the name and the arguments of the function and execute the corresponding function in your application.\n    - if No, the model has provided a direct text response to the prompt.\n4. If a function was executed, we capture the result and send it back to the model in a subsequent turn of conversation. And it will use the result to generate a response that incorporates the information from the function call.\n\nNow note that this process can be repeated over multiple turns. So this can allow for complex interactions and workflows. It's possible that even after providing the function call output, the model decides that it needs to use another function. And then it continues on using the calling function until it's satisfied. And the model can also support calling multiple functions in a single turn, which is called parallel function calling. And in sequence, which is called compositional function calling.\n\n# Example \nLet’s walk through a simple—but very representative—example of how function calling works.\nImagine you’re building a AI system that, for this really simple example lets that it  helps users ask questions about population estimates.\n\n\nBehind the scenes, we already have systems that store this kind of data, Business Registar, Building Registar, any sort of database, or it can be an external API.\n\nFor now, let’s pretend we have a Python function that just returns population counts based on a lookup. It’s stand in for our real data source.\n\nIn a real deployment, get_population_estimate could easily wrap:\n- A call to the StatsCan API,\n- A SQL query on a secure census warehouse,\n- Or a simple pandas.read_csv() from a shared directory.\n\nWhat matters is that we’ve exposed the function and described its signature in a way that the model understands—now, the model can decide when to call it and how to call it, but it won’t hallucinate the output.\n\n\n**Step 1 – Implement and document the function**\n\n**Step 2 – Publish the function schema to the model**\n\nWhen you create a chat completion you supply a tools list. Each entry is a JSON description of a function:\n\n**Step 3 – Start the conversation**:\nKick off a chat session with the tools attached.\n\n\n**Step 4 – Model emits a function call**\nIf LLM decides the function is relevant, the first response in choices[0].message will include a tool_calls array:\n\n\n``` python\n[\n    ChatCompletionMessageToolCall(id='call_aFCdjVWwL4K9rN0bZzFfgTpx',\n    function=Function(arguments='{\"geography\":\"Ontario\",\"year\":2021}',\n                      name='get_population_estimate'), type='function')]\n```\n\n**Step 5 – Execute the function**:\n\nYour code parses the arguments, runs the function, and obtains a result:\n\n**Step 6 – Feed the result back to the model**\n\nAppend two new messages to the running list:\n\n1. The original function-call message returned by the model (so GPT keeps its own conversational state).\n2. A tool-role message whose content is the JSON result and whose tool_call_id matches the earlier id.\n\n\nStep 7 – Ask the model to finish the answer\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"highlight-style":"github","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.37","theme":{"light":"cosmo","dark":"cyborg"},"google-fonts":["Inter:400,500,700","Merriweather:400,700"],"toc-float":{"collapsed":false,"smooth-scroll":true},"toc-expand":3,"toc-title":"Table of Contents","title":"Introduction to LLMs","date":"2025-05-29","author":"Hanan Ather","description":"An introduction to Large Language Models and their applications","categories":["AI","Machine Learning","LLMs"],"code-summary":"Show code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}