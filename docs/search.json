[
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "üìù",
    "section": "",
    "text": "Introduction to LLMs\n\n\n\n\n\nAn introduction to Large Language Models and their applications\n\n\n\n\n\nMay 29, 2025\n\n\nHanan Ather\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/introduction-to-llms/index.html",
    "href": "posts/introduction-to-llms/index.html",
    "title": "Introduction to LLMs",
    "section": "",
    "text": "How many of you have had a conversation like this with ChatGPT recently?\n\nUser: \"Analyze the trends in our Q3 survey data. We had 15,000 responses \nabout housing affordability across different provinces.\"\n\nChatGPT: \"Based on your description, here are some key trends I'd expect \nto see in Q3 housing affordability data:\n\n1. Regional variations likely show BC and Ontario with highest concerns\n2. Age demographics probably reveal millennials most affected\n3. Income brackets under $75K likely report significant challenges\n4. Urban vs rural differences may show urban areas more impacted\n\nTo get specific insights, I'd recommend looking at cross-tabulations \nbetween income, location, and reported affordability stress levels...\"\n\nThis highlights a fundamental challenge we face in 2025: LLMs are incredibly smart, but they‚Äôre designed for human conversation, not system integration\nIn our daily work at StatCan, we don‚Äôt just need insights we need those insights to flow into our data pipelines, our reporting systems, our analysis workflows. We need to bridge this gap between conversational AI and operational systems.\nAPI versions of these same LLMs can eliminate this translation step entirely. Instead of getting human-readable text that you then have to interpret and act upon, you can get: - Structured data that flows directly into your analysis tools - Function calls that automatically trigger the right actions in your systems - Formatted outputs that match your reporting requirements exactly\nBefore: \nAsk ChatGPT question ‚Üí 2. Read response ‚Üí 3. Interpret manually ‚Üí 4. Write code/queries ‚Üí 5. Execute actions ‚Üí 6. Format results\n\n\n\nimage.png\n\n\nAfter: Send API request ‚Üí 2. LLM returns structured data + triggers actions ‚Üí 3. Results automatically formatted and delivered\nThe key insight is this: The same AI capabilities you use in chat can be repurposed as intelligent middleware in your data systems. Let me show you exactly what this looks like in practice, starting with a simple example that will make the concept crystal clear.",
    "crumbs": [
      "Home",
      "My Writing",
      "Introduction to LLMs"
    ]
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#conversation-state",
    "href": "posts/introduction-to-llms/index.html#conversation-state",
    "title": "Introduction to LLMs",
    "section": "Conversation state",
    "text": "Conversation state\nThis naturally leads us to the idea of how to manage conversation state during a model interation. There are a few ways to manage conversation state, which is important for preserving information across multiple messages or turns in a conversation.\n\nManually manage the conversation state\nGenerally for most APIs each text generation request is indepent and stateless (unless we are specific APIs like the Assistants API), but we can still implement multi-turn conversations by providing additional messages as parameters to our text generation request.\n\n\nShow code\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What does Statistics Canada do?\"},\n        {\"role\": \"assistant\", \"content\": \"Statistics Canada is the national statistical office that collects, analyzes, and publishes statistical information about Canada's economy, society, and environment.\"},\n        {\"role\": \"user\", \"content\": \"What are their main survey types?\"},\n    ],\n)\n\nprint(response.choices[0].message.content)\n\n\nStatistics Canada conducts various types of surveys, which can be broadly categorized into the following main types:\n\n1. **Household Surveys**: These surveys gather data on the population, including labor force participation, income, health, education, and housing. Examples include the Labour Force Survey and the Census of Population.\n\n2. **Business Surveys**: These surveys collect information on the economic activities of businesses, including production, sales, and employment. Examples include the Annual Survey of Manufacturing and the Canadian Survey on Business Conditions.\n\n3. **Agricultural Surveys**: These surveys collect data on various aspects of agriculture, including crop production, livestock, and farm management practices. An example is the Census of Agriculture.\n\n4. **Census**: The Census of Population is conducted every five years and provides a comprehensive snapshot of the demographic characteristics of the Canadian population.\n\n5. **Health Surveys**: These surveys collect data on the health status, lifestyle, and use of health services by Canadians. An example is the Canadian Community Health Survey.\n\n6. **Environmental Surveys**: These surveys gather information on environmental factors, such as pollution, water use, and energy consumption. An example is the Households and the Environment Survey.\n\nThese surveys help inform public policy, support economic planning, and contribute to an understanding of Canadian society and its various dynamics.\n\n\nBy using alternating user and assistant messages, we can capture the previous state of conversation in one request to the model. To manually share context across generated responses, you include the model‚Äôs previous response output as input and append that input to your next request.\nIn the following example, we ask the model about Canadian demographics, followed by a request for provincial data. Appending previous responses to new requests in this way helps ensure conversations feel natural and retain the context of previous interactions.\n\n\nShow code\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nhistory = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the Canadian Census and how often does it occur?\"\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=history,\n)\n\nprint(response.choices[0].message.content)\n\nhistory.append(response.choices[0].message)\nhistory.append({ \"role\": \"user\", \"content\": \"What's the difference between the short and long form census?. Be concise and explain it be in less than 50 words\" })\n\nsecond_response = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=history,\n)\n\nprint(second_response.choices[0].message.content)\nprint(second_response.choices[0].message.content)\n\n\nThe Canadian Census is a national program conducted by Statistics Canada to collect comprehensive data on the country's population and housing. This data is crucial for government planning, policy-making, and the allocation of resources and services. The census gathers information on a variety of demographic, social, economic, and cultural aspects, such as age, sex, marital status, household relationships, language, employment, income, and housing conditions.\n\nThe Canadian Census occurs every five years, in years ending in ‚Äú1‚Äù and ‚Äú6.‚Äù The most recent census took place in 2021. Every ten years, in the census years ending in \"1\", a more detailed census known as the \"long-form census\" is distributed to a sample of the population. This long-form census collects additional information that provides insights into societal trends and changes. Participation in the census is mandatory, ensuring a high response rate and accuracy of the data collected.\nThe short-form census collects basic demographic data from all households, while the long-form census, sent to a sample, gathers detailed socio-economic information such as education, employment, and language, providing deeper insights into Canada's population and living conditions.\nThe short-form census collects basic demographic data from all households, while the long-form census, sent to a sample, gathers detailed socio-economic information such as education, employment, and language, providing deeper insights into Canada's population and living conditions.\n\n\n\n\nIncluding relevant context information\nIts often useful to include additional context information the model can use to generate a response within the prompt you give to the model. There are few common resons to do this: - to give the model access to any data data outside the data set the model was trained on. - to constrain the model‚Äôs response to a specific set of resources that you have determined will be most beneficial\nThis technique of adding additional releveant context to the model generation request is sometimes called retrieval augmented generation (RAG). You can add additional context to the prompt in many different ways, from querying from a vector database and including the text back into the prompt, or by APIs provide a bult-in file search tool to generate content based on uploaded documents.\n\n\nManaging the context window\nModels can only handle so much data within the context they consider during a generation request. (Mentioned needle in haystack metric). This memory limit is called a context window, which is defined in terms of tokens (chunks of data you pass in, from text, images or audio)\nUnderstanding the context window allows us to successfully create threaded conversations and manage state across model interactions.\nThe context window is the maximum number of tokens that be used in a single request. The max tokens include input, output and reasoning token.\nAs the inputs become more complex or we include more turns in a conversation, we will need to consider both output token and context window limits. Model input and outputs are metered in tokens, which at high level prased from inputs to analyze their content and intent and assembled to render logical outputs.\n\nOutput tokens are tokens generated by a model in response to a prompt. Each model has different limits for output tokens.\nA context window describes the total tokens that can be used for both input and output tokens (and for some models reasoning tokens)\n\nIf we create a very large prompt, often by including extra context, data, or examples of the model, we can run the risk of exceeding the allocated context window for a model, and this can result in truncated outputs.",
    "crumbs": [
      "Home",
      "My Writing",
      "Introduction to LLMs"
    ]
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#core-idea",
    "href": "posts/introduction-to-llms/index.html#core-idea",
    "title": "Introduction to LLMs",
    "section": "Core Idea",
    "text": "Core Idea\nThe core idea is that we can configure our LLM API for structured output instead of unstructured text. Period. This allows precise extraction and standardization of information for further processing. Period. For example, we can use the structured output to extract information from our resumes or standardize them to build a structured database. Let‚Äôs go over a couple of examples.\nNow, there are a couple of ways to get structured outputs out of a model. The first one that was introduced when structured outputs became a thing back 2023 was when I first discovered them. Structured outputs were a feature that ensured that a model would always generate responses that adhere to a JSON schema. So at that time, we had to give a JSON schema if you wanted to get structured outputs. However now, and more recently, APIs like for example, the OpenAI API just has structured outputs as a built-in feature. And in addition to supporting just JSON schema in the REST API, the OpenAI SDKs for Python and JavaScript also make it easy to define object schema using Pydantic and Zod if you‚Äôre using TypeScript respectively. So let me give an example of how you can extract information from unstructured output that conforms to a schema defined in code.\n\n\nGenerating enum values\nBefore we explore how enums work with structured outputs, let‚Äôs clarify what enums are in programming languages, particularly Python.\nAn enum (short for enumeration) is a way to create a set of named constants that represent a fixed collection of related values. Think of it as defining a specific list of allowed options that a variable can take.\nfrom enum import Enum\n\nclass Status(Enum):\n    PENDING = \"pending\"\n    APPROVED = \"approved\" \n    REJECTED = \"rejected\"\n    IN_REVIEW = \"in_review\"\n\nIn this example, Status can only be one of these four predefined values. You can‚Äôt accidentally set it to ‚Äúaproved‚Äù (misspelled) or ‚Äúmaybe‚Äù ‚Äì it must be one of the defined options.\nEnums become incredibly valuable when working with LLMs in real-world applications because they solve fundamental problems with unconstrained text generation.\nWhen LLMs generate free-form text responses, they operate in an infinite possibility space. Even for simple categorical decisions, models might express the same concept through countless linguistic variations, synonyms, or formatting differences. This variability creates a mismatch between the deterministic requirements of software systems and the inherently creative nature of language models.\nEnums transform open-ended generation into constrained selection. Instead of choosing from infinite possibilities, the model must map its understanding to one of your predefined categories.\nBy forcing the model to use your exact terminology, enums ensure that conceptually identical responses are represented identically in your system. The model‚Äôs internal understanding gets translated into your application‚Äôs vocabulary.\nDeterministic Interfaces: Enums create predictable contracts between your LLM and downstream systems. Rather than parsing and interpreting varied text outputs, your application receives known constants that can be handled with simple conditional logic.\nData Integrity at Scale: In production environments, consistency compounds over time. Enums prevent the gradual degradation of data quality that occurs when small variations accumulate across thousands or millions of LLM interactions.",
    "crumbs": [
      "Home",
      "My Writing",
      "Introduction to LLMs"
    ]
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#how-function-calling-works",
    "href": "posts/introduction-to-llms/index.html#how-function-calling-works",
    "title": "Introduction to LLMs",
    "section": "How Function Calling Works",
    "text": "How Function Calling Works\nSo, as we will see, function calling just simply builds on this idea of structured outputs, where function calling involves a structured interaction between your application, the model, and external functions. Here‚Äôs a breakdown of the steps:\nStep 1: Define Function Declaration\nWe define a function and its declaration within our application code that allows the user to set light And make API requests So this function could hypothetically call external services or APIs for instance\n\nDefine function declarations. So we define the function declaration in our application code. Function declarations describe the function‚Äôs name, parameters, and purpose to the model.\nCall the LLM with function declarations. Send user prompt along with the function declarations to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\nExecute function code, bracket R responsibility, bracket close. The model does not execute the function itself. Is your application‚Äôs responsibility to process the response back and check for function call?\n\nIf yes, then extract the name and the arguments of the function and execute the corresponding function in your application.\nif No, the model has provided a direct text response to the prompt.\n\nIf a function was executed, we capture the result and send it back to the model in a subsequent turn of conversation. And it will use the result to generate a response that incorporates the information from the function call.\n\nNow note that this process can be repeated over multiple turns. So this can allow for complex interactions and workflows. It‚Äôs possible that even after providing the function call output, the model decides that it needs to use another function. And then it continues on using the calling function until it‚Äôs satisfied. And the model can also support calling multiple functions in a single turn, which is called parallel function calling. And in sequence, which is called compositional function calling.",
    "crumbs": [
      "Home",
      "My Writing",
      "Introduction to LLMs"
    ]
  }
]